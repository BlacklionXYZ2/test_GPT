import torch
import torch.nn as nn
import torch.nn.functional as F
import os, math

#///////////////////////////////////////////////////////////////
#DISCLAIMER:
#I did not make this code, it was generated by chatGPT as an example to 
#help me understand how the code is implemented
#///////////////////////////////////////////////////////////////

def save(model, optimiser, step, path = 'save_state.pth'):
    torch.save({
        'step': step,
        'model_state': model.state_dict(),
        'optimiser_state': optimiser.state_dict()
        }, path)
    
def load(model, optimiser, path = 'save_state.pth'):
    global start_step
    if os.path.exists(path):
        checkpoint = torch.load(path, map_location = 'cpu')
        model.load_state_dict(checkpoint['model_state'])
        optimiser.load_state_dict(checkpoint['optimiser_state'])
        start_step = checkpoint['step']
        print(f'Resumed from checkpoint at step {start_step}')
        return start_step
    return 0

class miniGPT(nn.Module):
    def __init__(self, vocab_size = 100, d_model = 64, n_heads = 2, n_layers = 2, block_size = 64):
        super().__init__()
        self.token_emb = nn.Embedding(vocab_size, d_model)
        self.pos_emb = nn.Embedding(block_size, d_model)

        self.blocks = nn.ModuleList([TransformerBlock(d_model, n_heads) for _ in range(n_layers)])

        self.ln_f = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, vocab_size, bias = False)
        self.block_size = block_size

    def forward(self, idx):
        B, T = idx.shape
        assert T <= self.block_size
        pos = torch.arange(0, T, device = idx.device).unsqueeze(0)

        x = self.token_emb(idx) + self.pos_emb(pos)
        for block in self.blocks:
            x = block(x)
        x = self.ln_f(x)
        logits = self.head(x)
        return logits
    
class TransformerBlock(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.attn = nn.MultiheadAttention(d_model, n_heads, batch_first = True)
        self.ln1 = nn.LayerNorm(d_model)
        self.ff = nn.Sequential(
            nn.Linear(d_model, 4 * d_model),
            nn.ReLU(),
            nn.Linear(4 * d_model, d_model)
        )
        self.ln2 = nn.LayerNorm(d_model)

    def forward(self, x):
        attn_out, _ = self.attn(self.ln1(x), self.ln1(x), self.ln1(x), need_weights = False)
        x = x + attn_out
        x = x + self.ff(self.ln2(x))
        return x
    
class TextDataset(torch.utils.data.Dataset):
    def __init__(self, text, block_size, stoi, itos):
        self.block_size = block_size
        self.stoi = stoi
        self.itos = itos
        self.data = [stoi[ch] for ch in text]

    def __len__(self):
        return len(self.data) - self.block_size
    
    def __getitem__(self, idx):
        chunk = self.data[idx:idx + self.block_size + 1]
        x = torch.tensor(chunk[:-1], dtype = torch.long)
        y = torch.tensor(chunk[1:], dtype = torch.long)
        return x, y

def generate(model, idx, max_new_tokens):
    for _ in range(max_new_tokens):
        idx_cond = idx[:, -model.block_size:]
        logits = model(idx_cond)
        logits = logits[:, -1, :]
        probs = F.softmax(logits, dim = -1)
        next_token = torch.multinomial(probs, num_samples = 1)
        idx = torch.cat([idx, next_token], dim = 1)
    return idx

def main():
    global start_step
    if not os.path.exists('input.txt'):
        raise FileNotFoundError('put trainig data in input.txt')
    
    text = open('input.txt', 'r', encoding = 'utf-8').read()
    
    chars = sorted(list(set(text)))
    vocab_size = len(chars)
    stoi = {ch: i for i, ch in enumerate(chars)}
    itos = {i: ch for i, ch in enumerate(chars)}

    block_size = 64
    batch_size = 16

    dataset = TextDataset(text, block_size, stoi, itos)
    dataLoader = torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle = True)
    
    model = miniGPT(vocab_size = vocab_size, d_model = 64, n_heads = 2, n_layers = 2, block_size = block_size)
    optimiser = torch.optim.AdamW(model.parameters(), lr = 1e-3)
    loss_fn = nn.CrossEntropyLoss()

    start_step = load(miniGPT, optimiser)

    print('Training...')
    for epoch in range(50):
        for step, (x, y) in enumerate(dataLoader):
            global_step = epoch * len(dataLoader) + step + start_step
            logits = model(x)
            loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))
            optimiser.zero_grad()
            loss.backward()
            optimiser.step()

            if step % 100 == 0:
                print(f'Epoch {epoch}, step {step}, Loss {loss.item():.4f}')

            if global_step % 10000 == 0 and global_step > 0:
                save(model, optimiser, global_step)
                print(f'Checkpoint saved at step {global_step}')

        context = torch.zeros((1, 1), dtype = torch.long)
        out = generate(model, context, max_new_tokens = 200)[0].tolist()
        print('Sample://n', ''.join([itos[i] for i in out]))

        torch.save(model.state_dict(), 'miniGPT.pth')
        print('Training finished, model saved to miniGPT.pth')

if __name__ == '__main__':
    main()
    # text = open('input.txt', 'r', encoding = 'utf-8').read()
    # chars = sorted(list(set(text)))
    # context = torch.zeros((1, 1), dtype = torch.long)
    # out = generate(miniGPT(vocab_size = len(chars), d_model = 64, n_heads = 2, n_layers = 2, block_size = 64), context, max_new_tokens = 200)[0].tolist()
    # print('Sample://n', ''.join([{i: ch for i, ch in enumerate(chars)}[i] for i in out]))